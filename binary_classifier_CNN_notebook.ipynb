{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Elliott\\Anaconda3\\envs\\TensorFlow Testing\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Users\\Elliott\\Anaconda3\\envs\\TensorFlow Testing\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From E:\\Users\\Elliott\\Anaconda3\\envs\\TensorFlow Testing\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 68 samples, validate on 23 samples\n",
      "WARNING:tensorflow:From E:\\Users\\Elliott\\Anaconda3\\envs\\TensorFlow Testing\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      "68/68 [==============================] - 4s 52ms/sample - loss: 0.7632 - acc: 0.5000 - val_loss: 0.6737 - val_acc: 0.6087\n",
      "Epoch 2/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.7008 - acc: 0.5735 - val_loss: 0.6581 - val_acc: 0.6087\n",
      "Epoch 3/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.6755 - acc: 0.6471 - val_loss: 0.6319 - val_acc: 0.6087\n",
      "Epoch 4/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.6097 - acc: 0.6324 - val_loss: 0.6500 - val_acc: 0.6087\n",
      "Epoch 5/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.5604 - acc: 0.6176 - val_loss: 0.6298 - val_acc: 0.6087\n",
      "Epoch 6/40\n",
      "68/68 [==============================] - 1s 9ms/sample - loss: 0.5288 - acc: 0.6176 - val_loss: 0.6800 - val_acc: 0.6087\n",
      "Epoch 7/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.5035 - acc: 0.7059 - val_loss: 0.7830 - val_acc: 0.6087\n",
      "Epoch 8/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.4671 - acc: 0.6765 - val_loss: 0.8191 - val_acc: 0.9130\n",
      "Epoch 9/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.5051 - acc: 0.8235 - val_loss: 0.7976 - val_acc: 0.9130\n",
      "Epoch 10/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.4218 - acc: 0.8529 - val_loss: 0.8883 - val_acc: 0.8696\n",
      "Epoch 11/40\n",
      "68/68 [==============================] - 1s 9ms/sample - loss: 0.4002 - acc: 0.8971 - val_loss: 0.8285 - val_acc: 0.8696\n",
      "Epoch 12/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.3209 - acc: 0.9118 - val_loss: 0.9265 - val_acc: 0.8696\n",
      "Epoch 13/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.3510 - acc: 0.8235 - val_loss: 0.6692 - val_acc: 0.8696\n",
      "Epoch 14/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.3801 - acc: 0.8382 - val_loss: 0.6498 - val_acc: 0.8696\n",
      "Epoch 15/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.3271 - acc: 0.9265 - val_loss: 0.6735 - val_acc: 0.8696\n",
      "Epoch 16/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.2596 - acc: 0.8971 - val_loss: 0.8085 - val_acc: 0.8261\n",
      "Epoch 17/40\n",
      "68/68 [==============================] - 1s 9ms/sample - loss: 0.2632 - acc: 0.8824 - val_loss: 0.8158 - val_acc: 0.8696\n",
      "Epoch 18/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.1844 - acc: 0.9265 - val_loss: 0.8431 - val_acc: 0.9130\n",
      "Epoch 19/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.2121 - acc: 0.9118 - val_loss: 0.8347 - val_acc: 0.9130\n",
      "Epoch 20/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.2234 - acc: 0.8824 - val_loss: 0.8261 - val_acc: 0.9130\n",
      "Epoch 21/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.1754 - acc: 0.8971 - val_loss: 0.8241 - val_acc: 0.8696\n",
      "Epoch 22/40\n",
      "68/68 [==============================] - 1s 9ms/sample - loss: 0.1446 - acc: 0.9412 - val_loss: 0.8218 - val_acc: 0.9130\n",
      "Epoch 23/40\n",
      "68/68 [==============================] - 1s 9ms/sample - loss: 0.1854 - acc: 0.9118 - val_loss: 0.8230 - val_acc: 0.9565\n",
      "Epoch 24/40\n",
      "68/68 [==============================] - 1s 9ms/sample - loss: 0.2016 - acc: 0.9265 - val_loss: 0.8169 - val_acc: 0.8696\n",
      "Epoch 25/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.1152 - acc: 0.9853 - val_loss: 0.8709 - val_acc: 0.8696\n",
      "Epoch 26/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.1316 - acc: 0.9412 - val_loss: 0.8055 - val_acc: 0.9130\n",
      "Epoch 27/40\n",
      "68/68 [==============================] - 1s 9ms/sample - loss: 0.1008 - acc: 0.9559 - val_loss: 0.7921 - val_acc: 0.9130\n",
      "Epoch 28/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.1039 - acc: 0.9559 - val_loss: 0.7776 - val_acc: 0.9130\n",
      "Epoch 29/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0861 - acc: 0.9706 - val_loss: 0.7572 - val_acc: 0.9130\n",
      "Epoch 30/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0944 - acc: 0.9706 - val_loss: 0.7617 - val_acc: 0.9130\n",
      "Epoch 31/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0568 - acc: 0.9559 - val_loss: 0.7678 - val_acc: 0.9130\n",
      "Epoch 32/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0537 - acc: 0.9853 - val_loss: 0.8015 - val_acc: 0.9130\n",
      "Epoch 33/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0536 - acc: 0.9706 - val_loss: 0.8221 - val_acc: 0.9130\n",
      "Epoch 34/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0892 - acc: 0.9412 - val_loss: 0.7686 - val_acc: 0.9130\n",
      "Epoch 35/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0499 - acc: 0.9853 - val_loss: 0.8066 - val_acc: 0.9130\n",
      "Epoch 36/40\n",
      "68/68 [==============================] - 1s 9ms/sample - loss: 0.0456 - acc: 0.9853 - val_loss: 0.8042 - val_acc: 0.9130\n",
      "Epoch 37/40\n",
      "68/68 [==============================] - 1s 9ms/sample - loss: 0.0543 - acc: 0.9706 - val_loss: 0.7793 - val_acc: 0.9130\n",
      "Epoch 38/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0743 - acc: 0.9559 - val_loss: 0.7511 - val_acc: 0.9130\n",
      "Epoch 39/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0865 - acc: 0.9706 - val_loss: 0.7864 - val_acc: 0.9565\n",
      "Epoch 40/40\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 0.0942 - acc: 0.9559 - val_loss: 0.7785 - val_acc: 0.9130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ba2e3d2e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
    "import pickle\n",
    "\n",
    "# Load the pickle files we created in our load_images_notebook file\n",
    "X_f = pickle.load(open(\"X_features.pickle\", \"rb\"))\n",
    "y_l = pickle.load(open(\"y_labels.pickle\", \"rb\"))\n",
    "\n",
    "# Normalize the data\n",
    "X_f = tf.keras.utils.normalize(X_f, axis=1)\n",
    "\n",
    "# Construct the model\n",
    "model = Sequential()\n",
    "\n",
    "#Conv2D: Start with a convolutional layer\n",
    "#3x3 window\n",
    "model.add(Conv2D(64, (3,3), input_shape = X_f.shape[1:])) \n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Now we do it all again!\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# For good measure, put the data through a final dense layer at the end\n",
    "# Need to flatten the data from its 2D form for convolutional to 1D form\n",
    "# for the dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.5))\n",
    "# Final output layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "# In this instance we will use binary crossentropy since it is Clean vs. Contaminated\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"adam\",\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.25,\n",
    "                              patience=5, min_lr=0.001)\n",
    "model.fit(X_f, y_l, batch_size=32, validation_split=0.25, epochs=40, callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# model.save('basic_binary_classifier.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = tf.keras.models.load_model('basic_binary_classifier.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/43529931/how-to-calculate-prediction-uncertainty-using-keras?answertab=votes#tab-top\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "# uncertaintyFunction = K.function([model.layers[0].input, K.learning_phase()],\n",
    "#                [model.layers[-1].output])\n",
    "\n",
    "# def predict_with_uncertainty(f, x, n_iter=91):\n",
    "#     result = []\n",
    "#     for i in range(n_iter):\n",
    "#         result.append(f([x, 1]))\n",
    "    \n",
    "#     result = numpy.array(result)\n",
    "\n",
    "#     prediction = result.mean(axis=0)\n",
    "#     uncertainty = result.var(axis=0)\n",
    "#     return prediction, uncertainty\n",
    "# prediction, uncertainty = predict_with_uncertainty(uncertaintyFunction, X_f)\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 1s 6ms/sample - loss: 0.2426 - acc: 0.9780\n",
      "0.24261264290128434 0.978022\n"
     ]
    }
   ],
   "source": [
    "# new_model = tf.keras.models.load_model('E:/Users/Elliott/dev/aquavision_project/basic_binary_classifier.model')\n",
    "val_loss, val_acc = model.evaluate(X_f, y_l)\n",
    "print(val_loss, val_acc)\n",
    "np_test = np.array(X_f)\n",
    "# print(np_test)\n",
    "# Predict wants a numpy array as its parameter\n",
    "predictions = model.predict(X_f)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for the image at index 0 is: [1.]\n",
      "The correct value for the image at index 0 is: 1\n",
      "68\n",
      "78\n",
      "numCorr: 89\n",
      "Accuracy is: 0.978021978021978\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "k = 0\n",
    "print(\"The prediction for the image at index {} is: {}\".format(k, np.rint(predictions[k])))\n",
    "print(\"The correct value for the image at index {} is: {}\".format(k, y_l[k]))\n",
    "\n",
    "numCorr = 0\n",
    "for k in range(0, len(predictions)):\n",
    "    if(np.rint(predictions[k]) == y_l[k]):\n",
    "        numCorr+=1\n",
    "    else:\n",
    "        print(k)\n",
    "#         Print nums of images we didn't get\n",
    "#         Indexes 68 and 78 (#69 and 79) are consistently missed in every single version of this model. \n",
    "#         Index 6 (#7) also causes problems.\n",
    "print(\"numCorr: {}\".format(numCorr))\n",
    "print(\"Accuracy is: {}\".format(numCorr/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
